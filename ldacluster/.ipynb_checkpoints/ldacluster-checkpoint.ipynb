{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA (Latent Dirichlet Allocation)\n",
    "주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지에 대한 확률 모형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA) with Python ... review\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mother', 'spend', 'lot', 'time', 'drive', 'brother', 'around', 'basebal', 'practic']\n"
     ]
    }
   ],
   "source": [
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpora.Dictionary를 하면 어떻게 될까?\n",
    "\n",
    "결과가 궁금하잖아요?\n",
    "\n",
    "### corpora.dictionary – Construct word<->id mappings\n",
    "https://radimrehurek.com/gensim/corpora/dictionary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 'mother'),\n",
       " (0, 'like'),\n",
       " (3, 'eat'),\n",
       " (26, 'feel'),\n",
       " (11, 'practic'),\n",
       " (19, 'blood'),\n",
       " (12, 'spend'),\n",
       " (16, 'expert'),\n",
       " (30, 'say'),\n",
       " (14, 'may'),\n",
       " (8, 'time'),\n",
       " (24, 'often'),\n",
       " (4, 'good'),\n",
       " (22, 'perform'),\n",
       " (31, 'profession'),\n",
       " (18, 'health'),\n",
       " (13, 'pressur'),\n",
       " (1, 'brother'),\n",
       " (2, 'brocolli'),\n",
       " (10, 'lot'),\n",
       " (27, 'well'),\n",
       " (21, 'increas'),\n",
       " (6, 'drive'),\n",
       " (23, 'school'),\n",
       " (7, 'basebal'),\n",
       " (15, 'suggest'),\n",
       " (28, 'better'),\n",
       " (9, 'around'),\n",
       " (17, 'tension'),\n",
       " (25, 'seem'),\n",
       " (29, 'never'),\n",
       " (20, 'caus')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d for d in dictionary.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 2), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 느낌이겠네요\n",
    "\n",
    "(like - 1), (brother - 1), (brocolli - 2), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.058*\"brocolli\" + 0.058*\"good\" + 0.057*\"mother\"'), (1, '0.074*\"health\" + 0.045*\"pressur\" + 0.044*\"drive\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고로 ldamodel를 사용할때마다 결과가 달라요...\n",
    "\n",
    "그래서 이 밑에도 매번 달라요..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models.ldamodel – Latent Dirichlet Allocation\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.019716285673529226)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_term_topics(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.019716285673529226)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_term_topics('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.057498119807046166)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_term_topics('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.057498119807046166)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_term_topics(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저는 get_term_ropics에서 나오는 결과와 print_topics로 나오는 결과가 같을 줄 알았어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.066167506523890821),\n",
       " (2, 0.066166423202059232),\n",
       " (18, 0.066160493327345027),\n",
       " (13, 0.047241145589364181),\n",
       " (3, 0.047231029040298987),\n",
       " (6, 0.046898627821264922),\n",
       " (5, 0.046873646654270983),\n",
       " (1, 0.046869765381129726),\n",
       " (23, 0.02831345916223307),\n",
       " (25, 0.028313237617031993)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_topic_terms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.066167506523890821),\n",
       " (2, 0.066166423202059232),\n",
       " (18, 0.066160493327345027),\n",
       " (13, 0.047241145589364181),\n",
       " (3, 0.047231029040298987),\n",
       " (6, 0.046898627821264922),\n",
       " (5, 0.046873646654270983),\n",
       " (1, 0.046869765381129726),\n",
       " (23, 0.02831345916223307),\n",
       " (25, 0.028313237617031993),\n",
       " (28, 0.028313233175789976),\n",
       " (27, 0.0283131807788474),\n",
       " (26, 0.028313168743974312),\n",
       " (24, 0.02831262685919838),\n",
       " (22, 0.028312387000101014),\n",
       " (29, 0.028311956167798611),\n",
       " (14, 0.028297799292713754),\n",
       " (17, 0.028296543968432359),\n",
       " (16, 0.028295786511430283),\n",
       " (19, 0.028295008377450519),\n",
       " (0, 0.028294632195803134),\n",
       " (21, 0.028293595394571797),\n",
       " (20, 0.028292767304627191),\n",
       " (15, 0.028292763325622972),\n",
       " (30, 0.028267903678537074),\n",
       " (31, 0.028267707278266666),\n",
       " (9, 0.0094991239283826953),\n",
       " (7, 0.0094989906818570731),\n",
       " (8, 0.0094989372184822052),\n",
       " (12, 0.0094988804743121741),\n",
       " (11, 0.0094988736788426745),\n",
       " (10, 0.0094987996460688204)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_topic_terms(0, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_topic_terms로 나온 단어의 수치는 모두 더하면 1. 보정이 들어간걸 아닐까 짐작..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.066*\"good\" + 0.066*\"brocolli\" + 0.066*\"health\" + 0.047*\"pressur\" + 0.047*\"eat\" + 0.047*\"drive\" + 0.047*\"mother\" + 0.047*\"brother\" + 0.028*\"school\" + 0.028*\"seem\"'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.94317242265993106), (1, 0.056827577340068915)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.94317058299760514), (1, 0.05682941700239489)],\n",
       " [(0, 0.05624642575151062), (1, 0.94375357424848938)],\n",
       " [(0, 0.94867615728035593), (1, 0.051323842719644054)],\n",
       " [(0, 0.95321989050939382), (1, 0.046780109490606153)],\n",
       " [(0, 0.92474622253430605), (1, 0.075253777465693991)]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ldamodel[c] for c in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 얘네들도 다 더하면 1..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "무튼\n",
    "\n",
    "임의의 새로운 문장으로 테스트를 해보고 싶다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.82634397257500436), (1, 0.17365602742499564)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'i like eat'\n",
    "raw = test.lower()\n",
    "tokens = tokenizer.tokenize(raw)\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "ldamodel[dictionary.doc2bow(stemmed_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.5), (1, 0.5)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'the quick brown fox jumps over the lazy dog choker'\n",
    "raw = test.lower()\n",
    "tokens = tokenizer.tokenize(raw)\n",
    "stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "ldamodel[dictionary.doc2bow(stemmed_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.doc2bow(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
